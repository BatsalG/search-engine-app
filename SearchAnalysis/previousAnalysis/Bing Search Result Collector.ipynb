{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca763773",
   "metadata": {},
   "source": [
    "## Data Collection for Bing\n",
    "### Scraper that fetches the top 10 articles from Bing News for the given keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36cc2b",
   "metadata": {},
   "source": [
    "#### This notebook uses newspaper3k and beautifulsoup to extract the following features:\n",
    "* Title of the article\n",
    "* URL of the article\n",
    "* Published Date\n",
    "* Publisher\n",
    "* Description shown in the Result Page\n",
    "* Entire Article as a HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bdafeb",
   "metadata": {},
   "source": [
    "By default there are **30 keywords** for which the data is collected. These must be located in the same directory as this notebook, and be named as 'Capstone Keywords.csv'. The name can be change in the **Code Cell 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the two libraries using pip. The same can be done using conda prompt.\n",
    "# Uncomment the following lines to install the packages.\n",
    "\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451081d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries for the notebook.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from newspaper import Article\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccca29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list of keywords to analyze\n",
    "keyword_list = []\n",
    "\n",
    "# CSV file to access the keywords to analyze\n",
    "csv_for_keywords = 'Capstone Keywords.csv'\n",
    "\n",
    "# Access the keywords from the CSV file and create a list with it.\n",
    "with open(csv_for_keywords, newline='') as keywords_initial:\n",
    "    reader = csv.reader(keywords_initial)\n",
    "    for row in reader:\n",
    "        keyword_list.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9cb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data for inserting into the URL.\n",
    "keyword_for_query = []\n",
    "for keyword in keyword_list:\n",
    "    add_plus = keyword.replace(\" \", \"+\")\n",
    "    final_query = add_plus.replace(\"-\", \"+\")\n",
    "    keyword_for_query.append(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f589cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new CSV file to store the results\n",
    "file_write = open('raw_bing.csv', 'w', newline='', encoding='UTF8')\n",
    "writer = csv.writer(file_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036b6ff",
   "metadata": {},
   "source": [
    "### Main Section\n",
    "The following code cell extracts the top 10 results from Bing News, and inserts them to a CSV File.\n",
    "#### After running the cell, there should be a CSV file named \"raw_bing.csv\" in the working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a46fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Data from Bing:  70%|███████████████████████████████████████▉                 | 21/30 [03:24<01:24,  9.34s/it]C:\\Users\\batsa\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1213: UnknownTimezoneWarning: tzname IST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "Fetching Data from Bing:  93%|█████████████████████████████████████████████████████▏   | 28/30 [04:33<00:19,  9.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Header for the CSV File.\n",
    "header_csv = ['keyword', 'title', 'url', 'published_date', 'publisher', 'description', 'entire_doc']\n",
    "\n",
    "# Write the header into the newly created CSV file.\n",
    "writer.writerow(header_csv)\n",
    "\n",
    "# Main part of the scraper\n",
    "try:\n",
    "    # Fetch data for each keyword.\n",
    "    for keyword in tqdm(range(len(keyword_for_query)), desc = \"Fetching Data from Bing\"):\n",
    "        # Make the request to the given url.\n",
    "        page = requests.get('https://www.bing.com/news/search?q='+ keyword_for_query[keyword] +'&cc=US')\n",
    "        \n",
    "        # Create a BS4 object and find all the required values.\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        result_title = soup.find_all('div', class_='t_t')\n",
    "        result_desc = soup.find_all('div', class_='snippet')\n",
    "        result_link = soup.find_all('a', class_='title')\n",
    "        result_source = soup.find_all('div', class_='source')\n",
    "        \n",
    "        # Change the value to tweak how many results is to be extracted.\n",
    "        # By default, the value is 10.\n",
    "        for i in range(10):\n",
    "            # Insert the keyword into the list.\n",
    "            temp_list_results = []\n",
    "            temp_list_results.append(keyword_list[keyword])\n",
    "            if temp_list_results[0] not in keyword_list:\n",
    "                continue\n",
    "            \n",
    "            # Get the title and link of the article.\n",
    "            temp_list_results.append(result_title[i].get_text())\n",
    "            temp_list_results.append(result_link[i]['href'])\n",
    "            if temp_list_results[2][:4] != \"http\":\n",
    "                continue\n",
    "            # Conver the date into a relevant format.\n",
    "            date_ = result_source[i].findChildren(\"span\")[2].get_text()\n",
    "            minute = 0\n",
    "            hour = 0\n",
    "            day = 0\n",
    "            if date_[-1] == \"m\":\n",
    "                minute = int(date_[:-1])\n",
    "            elif date_[-1] == \"h\":\n",
    "                hour = int(date_[:-1])\n",
    "            elif date_[-1] == \"d\":\n",
    "                day = int(date_[:-1])\n",
    "            else:\n",
    "                pass\n",
    "            date_published = datetime.today() - timedelta(days = day, hours=hour, minutes=minute)\n",
    "            date_published = date_published.strftime('%a, %d %b %Y %X')\n",
    "            temp_list_results.append(date_published)\n",
    "\n",
    "            # Get the description and the entire text.\n",
    "            temp_list_results.append(result_source[i].findChild().get_text())\n",
    "            temp_list_results.append(result_desc[i].get_text())\n",
    "            try:\n",
    "                # Access the text from the URL.\n",
    "                article = Article(temp_list_results[2])\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                temp_list_results.append(article.text)\n",
    "            except:\n",
    "                temp_list_results.append('')\n",
    "            # Insert the list we created into the CSV file.\n",
    "            writer.writerow(temp_list_results)\n",
    "    # Close the file once complete.\n",
    "    file_write.close()\n",
    "# Close the file prematurely, if there is a fatal error.\n",
    "except:\n",
    "    file_write.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
